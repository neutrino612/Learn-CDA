总结：决策树
八个参数：Criterion，两个随机性相关的参数（random_state，splitter），五个剪枝参数（max_depth,
min_samples_split，min_samples_leaf，max_feature，min_impurity_decrease）
一个属性：feature_importances_
四个接口：fit，score，apply，predict



1.scikit-learn，又写作sklearn，是一个开源的基于python语言的机器学习工具包
它通过NumPy, SciPy和
Matplotlib等python数值计算的库实现高效的算法应用，并且涵盖了几乎所有主流机器学习算法。

2、《数据挖掘导论》
《机器学习》

3、决策树（Decision Tree）是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规
则，并用树状图的结构来呈现这些规则，以解决分类和回归问题

4、关键概念：节点
根节点：没有进边，有出边。包含最初的，针对特征的提问。 
中间节点：既有进边也有出边，进边只有一条，出边可以有很多条。都是针对特征的提问。 
叶子节点：有进边，没有出边，每个叶子节点都是一个类别标签。 
子节点和父节点：在两个相连的节点中，更接近根节点的是父节点，另一个是子节点。

5、决策树算法的核心是要解决两个问题：
1）如何从数据表中找出最佳节点和最佳分枝？
2）如何让决策树停止生长，防止过拟合？

6、模块sklearn.tree
这个模块总共包含五个类：
tree.DecisionTreeClassifier 分类树
tree.DecisionTreeRegressor 回归树
tree.export_graphviz 将生成的决策树导出为DOT格式，画图专用
tree.ExtraTreeClassifier 高随机版本的分类树
tree.ExtraTreeRegressor 高随机版本的回归树

7、sklearn的基本建模流程
（1）实例化，建立评估模型对象   实例化时，需要使用的参数
（2）通过模型接口训练模型
（3）通过模型接口提取需要的信息    （2）和（3）数据属性数据接口

8、用红酒数据集，决策树的参数
（1） criterion：决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标，叫做“不纯度”
    Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：
    1）输入”entropy“，使用信息熵（Entropy） 
    2）输入”gini“，使用基尼系数（Gini Impurity）

（2） random_state ：用来设置分枝中的随机模式的参数，在高维度时随机性会表现更明显，低维度的数据
（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。
（3）splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best"，决策树在分枝时虽然随机，但是还是会
优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random"，决策树在
分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。

当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。


剪枝参数：
（4）max_depth：限制树的最大深度，超过设定深度的树枝全部剪掉。这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。
（5）min_samples_leaf：min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分
枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生
（6）min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则
分枝就不会发生。
（7）max_features：限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃
（8）min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生

9、确认最优的剪枝参数：超参数的学习曲线，是一条以超参数的取值为横坐标，模型的度量指标为纵坐标的曲
线，它是用来衡量不同超参数取值下模型的表现的线
这么多参数，一个个画学习曲线？

（1）class_weight：完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。我们要使用class_weight参数对样本标签进行一定的均衡，给
少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_
weight_fraction_leaf这个基于权重的剪枝参数来使用




10、决策树的基本流程：
计算全部特征的不纯度，选取不纯度指标最优的特征来分枝，在第一个特征的分枝下，计算全部分枝的不纯度指标，选择剩下指标中的最优继续分枝


11、决策树在建树时，是靠优化节点来追求一棵优化的树，但最优的节点能够保证最优的树吗？  过拟合问题会出现
sklearn表示，既然一棵树不能保证最优，那就建更
多的不同的树，然后从中取最好的。怎样从一组数据集中建不同的树？在每次分枝时，不从使用全部特征，而是随
机选取一部分特征，从中选取不纯度相关指标最优的作为分枝用的节点。这样，每次生成的树也就不同了。


12、为了让决策树有更好的泛化性，我们要对决策树进行剪枝。剪枝策略对决策树的影响巨大，正确的剪枝策略是优化
决策树算法的核心。  详细见8中的参数

13、回归树：预测一些非分类的问题

14、回归树衡量分枝质量的指标，支持的标准有三种：
（1）输入"mse"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为
特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失
（2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差
（3）输入"mae"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失
属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心。

15、交叉验证是用来观察模型的稳定性的一种方法，我们将数据划分为n份，依次使用其中一份作为测试集，其他n-1份
作为训练集，多次计算模型的精确性来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此
用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量。

16、



1、集成学习（ensemble learning）是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通
过在数据上构建多个模型，集成所有模型的建模结果。是一种算法思想

2、在现在的各种算法竞赛中，随机森林，梯度提升树（GBDT），Xgboost等集成算法的身影也随处可见

3、集成算法的目标：集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或
分类表现。

4、集成评估器（ensemble estimator）：多个模型集成成为的模型
        基评估器（base estimator）：组成集成评估器的每个模型都叫做
通常来说，有三类集成算法：装袋法（Bagging）【模型独立，相互平行】，提升法（Boosting）【模型有序，逐渐提升】和stacking。

4-1、装袋法：核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。
装袋法的代表模型就是随机森林

4-2、提升法：基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本
进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树

5、sklearn中的集成算法模块ensemble
其中的类有很多：ensemble.RandomForestClassifier 随机森林分类和ensemble.RandomForestRegressor 随机森林回归

5-1、ensemble.RandomForestClassifier 随机森林分类，重要参数与决策树一致

5-2、n_estimators这是森林中树木的数量，即基评估器的数量。理论上越大模型效果越好，但任何模型都有边界

5-3、n_estimators的学习曲线，其值的变化对预测效果的影响

5-4、 random_state两个模块的不同之处，sklearn中的DecisionTreeClassifier自带随机性，所以随机森林中的树天生就都是不一样的。
决策树从最重要的特征中随机选择出一个特征来进行分枝，因此每次生成的决策树都不一样，这个功能由参数random_state控制

随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一
棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树

但这种做法的局限性是很强的，当我们需要成千上万棵树的时候，数据不一定能够提供成千上万的特征来让我们构
筑尽量多尽量不同的树。因此，除了random_state。我们还需要其他的随机性。

5-5、 bootstrap & oob_score
要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的
随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数。bootstrap参数默认True，代表采用这种有放回的随机抽样技术


袋外数据(out of bag data，简写为oob)：当n足够大时，这个概率收敛于1-(1/e)，约等于0.632。因此，会有约37%的训练数据被浪费掉，这些数据也可
以被用来作为集成算法的测试集。也就是说，在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可。


6、为什么随机森林分类树比决策树好
降低异常值所带来的影响：因为随机森林选取了部分数据建立了多个决策树，即使有个别决策树会因为异常值的影响导致预测不准确，
但预测结果是参考多个决策树得到的结果，降低了异常值带来的影响。

降低了过拟合的可能性，因为决策树是采用了所有的特征及样本，容易出现过拟合（即对训练样本有很好的效果，对测试集的效果很差）
随机森林是采用了部分样本的部分特征而构造的很多个决策树（采取的有放回抽样），特征和数据在单个决策树上变少了，降低了过拟合的可能性。

7、重要接口与属性
随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score，predict_proba接口
传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均
每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类

8、RandomForestRegressor随机森林回归树 的重要接口和参数
8-1、仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致：
回归树衡量分枝质量的指标，支持的标准有三种：

8-2、随机森林回归并没有predict_proba这个接口，因为对于回归来说，并不存在一个样本要被分到某个类别的概率问
题，因此没有predict_proba这个接口。

9、关于缺失值处理，填补的方法，使用sklearn.impute.SimpleImputer来轻松地将均值，中值，或者其他最常用的数值填补到数据中
案例：实际上，标签和特征是可以相互转换的，比如说，在一个“用地区，环境，附近学校数
量”预测“房价”的问题中，我们既可以用“地区”，“环境”，“附近学校数量”的数据来预测“房价”，也可以反过来，
用“环境”，“附近学校数量”和“房价”来预测“地区”。而回归填补缺失值，正是利用了这种思想

10、 机器学习中调参的基本思想
其一是因为，调参的方式总是根据数据的状况而定，所
以没有办法一概而论；其二是因为，其实大家也都没有特别好的办法。通过画学习曲线，或者网格搜索，我们能够探索到调参边缘
经验，来源于：1）非常正确的调参思路和方法，2）对模型评估指
标的理解，3）对数据的感觉和经验，4）用洪荒之力去不断地尝试。

11、泛化误差：在机器学习中，我们用来衡量模型在未知数据上的准确率的指标
观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，
而蓝色的线代表着数据本来的面貌。
偏差：模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离。在集成算法中，每个基评估器都会有
自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。
方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，
衡量模型的稳定性。模型越稳定，方差越低。


其中偏差衡量模型是否预测得准确，偏差越小，模型越“准”；而方差衡量模型每次预测的结果是否接近，即是说方
差越小，模型越“稳”；

